{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "[0 1 2 3 4 5] 6\n",
      "\n",
      "[0.         0.00392157 0.00784314 0.01176471 0.01568628 0.01960784] 6\n",
      "\n",
      "[0 1 2 3 4 5] 6\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/zekun/drivable/test/simpletest.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btlab.server/home/zekun/drivable/test/simpletest.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39munique(np\u001b[39m.\u001b[39marray(label)), \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(np\u001b[39m.\u001b[39marray(label))))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btlab.server/home/zekun/drivable/test/simpletest.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btlab.server/home/zekun/drivable/test/simpletest.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mone_hot(label, num_classes\u001b[39m=\u001b[39;49m\u001b[39m21\u001b[39;49m)\u001b[39m.\u001b[39;49mpermute(\u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btlab.server/home/zekun/drivable/test/simpletest.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m(label\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btlab.server/home/zekun/drivable/test/simpletest.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# print(label[label2=])\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "label_path = \"/home/zekun/drivable/data/bdd100k/labels/10k/ins_seg/bitmasks/val/82ad9d1c-1a5212c8.png\"\n",
    "# label_path = \"/home/zekun/drivable/data/bdd100k/labels/100k/drivable/masks/val/b1c66a42-6f7d68ca.png\"\n",
    "\n",
    "label = Image.open(label_path)\n",
    "print(np.array(label))\n",
    "print(np.unique(np.array(label)), len(np.unique(np.array(label))))\n",
    "print()\n",
    "\n",
    "def replace_255_with_20(label):\n",
    "    # label = np.array(label)\n",
    "    label[label==255] = 20\n",
    "    # return Image.fromarray(label)\n",
    "    return label\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        \n",
    "        transforms.Resize((512,1024), transforms.InterpolationMode.NEAREST),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.squeeze())\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "label = transform(label)\n",
    "print(np.unique(np.array(label)), len(np.unique(np.array(label))))\n",
    "print()\n",
    "# Convert label to tensor and convert from RGB to single channel (grayscale)\n",
    "label = (label*255).to(torch.int64)\n",
    "print(np.unique(np.array(label)), len(np.unique(np.array(label))))\n",
    "print()\n",
    "\n",
    "label = torch.nn.functional.one_hot(label, num_classes=21).permute(2, 0, 1).float()\n",
    "print(label.shape)\n",
    "# print(label[label2=])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True]\n"
     ]
    }
   ],
   "source": [
    "arr1 = [1,2,3]\n",
    "arr2 = [1,3,2]\n",
    "\n",
    "print(np.logical_and(arr1, arr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.dbhandler import JsonDBHandler\n",
    "\n",
    "handler = JsonDBHandler.create_db(\"/home/zekun/drivable/outputs/obj_det/db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "img = read_image(\"../data/bdd100k/images/10k/val/7d06fefd-f7be05a6.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = [preprocess(img)]\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = model(batch)[0]\n",
    "labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "box = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n",
    "                          labels=labels,\n",
    "                          colors=\"red\",\n",
    "                          width=4, font_size=30)\n",
    "im = to_pil_image(box.detach())\n",
    "im.save(\"ttt.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/zekun/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-12-11 Python-3.10.12 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'models.common.DetectMultiBackend'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(model))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(results))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Results\u001b[39;00m\n",
      "File \u001b[0;32m~/drivable/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:520\u001b[0m, in \u001b[0;36mDetectMultiBackend.forward\u001b[0;34m(self, im, augment, visualize)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, im, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, visualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# YOLOv5 MultiBackend inference\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     b, ch, h, w \u001b[38;5;241m=\u001b[39m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m  \u001b[38;5;66;03m# batch, channel, height, width\u001b[39;00m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mand\u001b[39;00m im\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m    522\u001b[0m         im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mhalf()  \u001b[38;5;66;03m# to FP16\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False, pretrained=True)\n",
    "\n",
    "# Images\n",
    "imgs = ['https://ultralytics.com/images/zidane.jpg']  # batch of images\n",
    "\n",
    "print(type(model))\n",
    "# Inference\n",
    "results = model(imgs)\n",
    "print(type(results))\n",
    "# Results\n",
    "results.print()\n",
    "# results.save()  # or .show()\n",
    "\n",
    "results.xyxy[0]  # img1 predictions (tensor)\n",
    "results.pandas().xyxy[0]  # img1 predictions (pandas)\n",
    "#      xmin    ymin    xmax   ymax  confidence  class    name\n",
    "# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\n",
    "# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\n",
    "# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\n",
    "# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': PosixPath('/home/zekun/drivable/tmp/dataset'), 'train': ['/home/zekun/drivable/tmp/dataset/images/train', '/home/zekun/drivable/tmp/dataset/images/val'], 'val': '/home/zekun/drivable/tmp/dataset/images/val', 'names': {0: 'pedestrian', 1: 'rider', 2: 'car', 3: 'truck', 4: 'bus', 5: 'train', 6: 'motorcycle', 7: 'bicycle', 8: 'traffic light', 9: 'traffic sign'}, 'yaml_file': '/home/zekun/drivable/test/yolo/bdd100k_yolo.yaml', 'nc': 10}\n"
     ]
    }
   ],
   "source": [
    "from ultralytics.utils import yaml_load\n",
    "from ultralytics.cfg import get_cfg\n",
    "from ultralytics.data.utils import check_det_dataset\n",
    "\n",
    "cfg = get_cfg(overrides={\"data\": \"/home/zekun/drivable/test/yolo/bdd100k_yolo.yaml\"})\n",
    "data = check_det_dataset(cfg.data)\n",
    "print(data)\n",
    "# print(yaml_load(\"/home/zekun/drivable/test/yolo/bdd100k_yolo.yaml\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drivable-kJxmeTfE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
